\documentclass[man]{apa7}
\usepackage[american]{babel}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\usepackage{csquotes}
\usepackage{titlepic}
\usepackage{graphicx}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage{lipsum}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{mybibfile.bib}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textit{Graduation Project}}
\rhead{\thepage}


\begin{document}
\begin{titlepage}
\centering
{\large \textbf{ Istanbul Technical University - Science and Letters Faculty}}\\
{\large \textbf{Mathematics Engineering Program}}\\[4\baselineskip]
\includegraphics[scale = 1.5]{itülogo.png}\\[6\baselineskip]
{\LARGE \textbf{Graduation Project}}\\[2\baselineskip]

{\Large Student Name: Arif Çakır}\\
{\large Student Number: 090190355}\\
{\large Course: MAT4091E}\\
{\large Advisor: Prof. Dr. Atabey Kaygun}\\
{\large Submission Date: \today}\\
\end{titlepage}
\tableofcontents
\pagebreak


\section{Introduction}
Language is a fundamental aspect of human communication and one of the most common ways to pass on information and culture throughout history. Due to its nature of encapsulating information, language is studied by linguists throughout the years. Thanks to this scientific foundation, machine learning, and computer science are also advanced upon this topic. Natural Language Processing (NLP) is the subfield of machine learning that studies the processing of human language by computers. One of the most popular NLP techniques in recent years is Word Embedding, which represents words as vectors in semantic space that allows applying mathematical operations on them to analyse. Word Embedding can be used on various subjects such as text classification, translation, and sentiment analysis with promising results. It is aimed in this paper to study several Word Embedding techniques and their applications. The aim is to create a model by applying various Word Embedding models and with the help of this model, describing the relationships and similarities of languages.\\
The rest of the paper is organised as follows: In Section 2, information about Word Embedding in general and several Word Embedding techniques are provided. Word Embedding techniques touched on in the paper are Term Frequency (TF), Inverse Document Frequency (IDF), Word2Vec, Global Vectors for Word Representation (GloVe), Bidirectional Encoder Representations from Transformers (BERT). This is followed by Section 3 which introduces the dataset {\large \textbf{don't forget the literature name}} by ----. After that, several Word Embedding models are applied to the dataset in Section 4, and the outputs of those models are analysed in Section 5. Finally paper ends with some discussion and a conclusion in Section 6.

\section{Word Embedding Techniques}
\lipsum[0-1]
\subsection{Term Frequency-Inverse Document Frequency}
Term Frequency (TF) is a word embedding technique...
\subsection{Word2Vec}
Word2Vec is a word embedding model...
\subsubsection{Skip-Gram Algorithm}
One of the Word2Vec model architectures is Skip-Gram Algorithm...
\subsubsection{Continuous Bag of Words}
The algorithm that does exact opposite of the Skip-Gram Algorithm is called Continuous Bag of Words (CBOW)...
\subsection{Global Vectors for Word Representation}
Global Vectors for Word Representation (GloVe)
\subsection{Bidirectional Encoder Representations from Transformers}
Created by researchers at Google in 2018, Bidirectional Encoder Representations from Transformers (BERT) is a family of masked-language models...
\section{Dataset}
\lipsum[0-1]
\subsection{Information About the Dataset}
\lipsum[0-1]
\subsection{Preparation of the Dataset}
\lipsum[0-1]
\section{Applying Models to the Dataset}
\lipsum[0-1]
\subsection{TF-IDF}
\lipsum[0-1]
\subsection{Skip-Gram Algorithm}
\lipsum[0-1]
\subsection{Continuous Bag of Words}
\lipsum[0-1]
\section{Analysis and Visualization}
\section{Conclusion}
\lipsum[0-1]
\end{document}