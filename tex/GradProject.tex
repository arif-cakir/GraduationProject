\documentclass[man]{apa7}
\usepackage[american]{babel}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\usepackage{csquotes}
\usepackage{titlepic}
\usepackage{graphicx}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage{lipsum}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{mybibfile.bib}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textit{Graduation Project}}
\rhead{\thepage}


\begin{document}
\begin{titlepage}
\centering
{\large \textbf{ Istanbul Technical University - Science and Letters Faculty}}\\
{\large \textbf{Mathematics Engineering Program}}\\[4\baselineskip]
\includegraphics[scale = 1.5]{itülogo.png}\\[6\baselineskip]
{\LARGE \textbf{Graduation Project}}\\[2\baselineskip]

{\Large Student Name: Arif Çakır}\\
{\large Student Number: 090190355}\\
{\large Course: MAT4091E}\\
{\large Advisor: Prof. Dr. Atabey Kaygun}\\
{\large Submission Date: \today}\\
\end{titlepage}
\tableofcontents
\pagebreak


\section{Introduction}
Language is a fundamental aspect in human communication and one of the most common ways to pass on information and culture throughout the history. Due to the its nature of encapsulating information, language is studied by linguists throughout the years to understand vocal manifestation of culture. Thanks to this scientific foundation, machine learning and computer science are also advanced upon this topic. Natural Language Processing (NLP) is the subfield of machine learning that studies on processing human language by computers. One of the most popular NLP technique in recent years is Word Embedding, which represents words as vectors in semantic space that allows applying mathematical operations on them to analyse. Word Embedding can be used on various subjects such as text classification, translation, sentiment analysis with promising results. 
In this paper, it is aimed to study several word embedding techniques and  their applications. The aim is creating a model by applying various word embedding models and with the help of this model, describing relation and similarities of languages. Section 2 
\section{Word Embedding Techniques}
\lipsum[0-1]
\subsection{Term Frequency-Inverse Document Frequency}
\lipsum[1]
\subsection{Word2Vec}
\lipsum[1]
\subsubsection{Skip-Gram Algorithm}
\subsubsection{Continuous Bag of Words}
\subsection{Global Vectors for Word Representation}
\subsection{Bidirectional Encoder Representations from Transformers}
\section{Dataset}
\subsection{Information About the Dataset}
\subsection{Preparation of the Dataset}
\section{Applying Models to the Dataset}
\subsection{TF-IDF and IDF}
\subsection{Skip-Gram Algorithm}
\subsection{Continuous Bag of Words}
\section{Analysis and Visualization}
\section{Conclusion}
\lipsum[1]
\end{document}