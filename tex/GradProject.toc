\babel@toc {american}{}\relax 
\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {section}{\numberline {2}Word Embedding Techniques}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Term Frequency-Inverse Document Frequency}{4}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Word2Vec}{4}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Skip-Gram Algorithm}{5}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Continuous Bag of Words}{5}{subsubsection.2.2.2}%
\contentsline {subsection}{\numberline {2.3}Global Vectors for Word Representation}{5}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Bidirectional Encoder Representations from Transformers}{5}{subsection.2.4}%
\contentsline {section}{\numberline {3}Dataset}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Information About the Dataset}{5}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Preparation of the Dataset}{5}{subsection.3.2}%
\contentsline {section}{\numberline {4}Applying Models to the Dataset}{5}{section.4}%
\contentsline {subsection}{\numberline {4.1}TF-IDF and IDF}{5}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Skip-Gram Algorithm}{5}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Continuous Bag of Words}{5}{subsection.4.3}%
\contentsline {section}{\numberline {5}Analysis and Visualization}{5}{section.5}%
\contentsline {section}{\numberline {6}Conclusion}{5}{section.6}%
